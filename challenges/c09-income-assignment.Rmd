---
title: "US Income"
author: "RICHARD LI"
date: 2023-4-7
output:
  github_document:
    toc: true
prerequisites:
  - e-stat09-bootstrap
---

*Purpose*: We've been learning how to quantify uncertainty in estimates through the exercises; now its time to put those skills to use studying real data. In this challenge we'll use concepts like confidence intervals to help us make sense of census data.

*Reading*: - [Using ACS Estimates and Margin of Error](https://www.census.gov/programs-surveys/acs/guidance/training-presentations/acs-moe.html) (Optional) - [Patterns and Causes of Uncertainty in the American Community Survey](https://www.sciencedirect.com/science/article/pii/S0143622813002518?casa_token=VddzQ1-spHMAAAAA:FTq92LXgiPVloJUVjnHs8Ma1HwvPigisAYtzfqaGbbRRwoknNq56Y2IzszmGgIGH4JAPzQN0) (Optional, particularly the *Uncertainty in surveys* section under the Introduction.)

<!-- include-rubric -->

# Grading Rubric

<!-- -------------------------------------------------- -->

Unlike exercises, **challenges will be graded**. The following rubrics define how you will be graded, both on an individual and team basis.

## Individual

<!-- ------------------------- -->

| Category    | Needs Improvement                                                                                                | Satisfactory                                                                                                               |
|------------------|-----------------------------|-------------------------|
| Effort      | Some task **q**'s left unattempted                                                                               | All task **q**'s attempted                                                                                                 |
| Observed    | Did not document observations, or observations incorrect                                                         | Documented correct observations based on analysis                                                                          |
| Supported   | Some observations not clearly supported by analysis                                                              | All observations clearly supported by analysis (table, graph, etc.)                                                        |
| Assessed    | Observations include claims not supported by the data, or reflect a level of certainty not warranted by the data | Observations are appropriately qualified by the quality & relevance of the data and (in)conclusiveness of the support      |
| Specified   | Uses the phrase "more data are necessary" without clarification                                                  | Any statement that "more data are necessary" specifies which *specific* data are needed to answer what *specific* question |
| Code Styled | Violations of the [style guide](https://style.tidyverse.org/) hinder readability                                 | Code sufficiently close to the [style guide](https://style.tidyverse.org/)                                                 |

## Due Date

<!-- ------------------------- -->

All the deliverables stated in the rubrics above are due **at midnight** before the day of the class discussion of the challenge. See the [Syllabus](https://docs.google.com/document/d/1qeP6DUS8Djq_A0HMllMqsSqX3a9dbcx1/edit?usp=sharing&ouid=110386251748498665069&rtpof=true&sd=true) for more information.

# Setup

<!-- ----------------------------------------------------------------------- -->

```{r setup}
library(tidyverse)
```

### **q1** Load the population data from c06; simply replace `filename_pop` below.

```{r q1-task}
## TODO: Give the filename for your copy of Table B01003
filename_pop <- "data/ACSDT5Y2018.B01003-Data.csv"

## NOTE: No need to edit
df_pop <-
  read_csv(
    filename_pop,
    skip = 2,
    col_names = c(
      "id",
      "geographic_area_name",
      "population_estimate",
      "population_moe"
    )
  )
df_pop
```

You might wonder why the `Margin of Error` in the population estimates is listed as `*****`. From the [documentation (PDF link)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj81Omy16TrAhXsguAKHTzKDQEQFjABegQIBxAB&url=https%3A%2F%2Fwww2.census.gov%2Fprograms-surveys%2Facs%2Ftech_docs%2Faccuracy%2FMultiyearACSAccuracyofData2018.pdf%3F&usg=AOvVaw2TOrVuBDlkDI2gde6ugce_) for the ACS:

> If the margin of error is displayed as '\*\*\*\*\*' (five asterisks), the estimate has been controlled to be equal to a fixed value and so it has no sampling error. A standard error of zero should be used for these controlled estimates when completing calculations, such as those in the following section.

This means that for cases listed as `*****` the US Census Bureau recommends treating the margin of error (and thus standard error) as zero.

### **q2** Obtain median income data from the Census Bureau:

-   `Filter > Topics > Income and Poverty > Income and Poverty`
-   `Filter > Geography > County > All counties in United States`
-   Look for `Median Income in the Past 12 Months` (Table S1903)
-   Download the 2018 5-year ACS estimates; save to your `data` folder and add the filename below.

```{r q2-task}
## TODO: Give the filename for your copy of Table S1903
filename_income <- "data/ACSST5Y2018.S1903-Data.csv"

## NOTE: No need to edit
df_income <-
  read_csv(filename_income, skip = 1) %>%
  rename(
    id = Geography,
  ) %>% 
  select(
    -contains("Annotation")
  )

df_income
```

Use the following test to check that you downloaded the correct file:

```{r q2-tests}
## NOTE: No need to edit, use to check you got the right file.
assertthat::assert_that(
  df_income %>%
    filter(id == "0500000US01001") %>%
    pull(`Estimate!!Percent Distribution!!FAMILY INCOME BY FAMILY SIZE!!2-person families`)
  == 45.6
)

print("Well done!")
```

This dataset is in desperate need of some *tidying*. To simplify the task, we'll start by considering the `\\d-person families` columns first.

### **q3** Tidy the `df_income` dataset by completing the code below. Pivot and rename the columns to arrive at the column names `id, geographic_area_name, category, income_estimate, income_moe`.

*Hint*: You can do this in a single pivot using the `".value"` argument and a `names_pattern` using capture groups `"()"`. Remember that you can use an OR operator `|` in a regex to allow for multiple possibilities in a capture group, for example `"(Estimate|Margin of Error)"`.

```{r q3-task}
df_working <-
  df_income %>%
  select(
    id,
    contains("Geographic"),
    contains("median") & matches("\\d-person families")
  ) %>%
  mutate(across(contains("median"), as.numeric))


df_working %>% glimpse()

df_q3 <- df_working %>%
  pivot_longer(
    names_pattern = "(Estimate|Margin of Error).*(\\d-person families)",
    names_to = c(".value", "category"),
    cols = -c("id", "Geographic Area Name")
  ) %>%
  rename(
    "geographic_area_name" = "Geographic Area Name",
    "income_estimate" = "Estimate",
    "income_moe" =  "Margin of Error"
  )


## NOTE: I KNOW THIS IS UGLY. I'M LEAVING IT AS A REMINDER TO MYSELF THAT IT CAN ALWAYS BE WORSE :P

# df_q3 <- df_working %>%
#   pivot_longer(
#     names_to = "dropping_this",
#     values_to = "income_moe",
#     cols = contains("Margin of Error")
#   ) %>%
#   pivot_longer(
#     names_to = "drop",
#     values_to = "income_estimate",
#     cols = contains("Estimate")
#   ) %>%
#   separate(
#     col = ("dropping_this"), 
#     into = c("drop_this", "category"),
#     sep = (-17)
#   ) %>%
#   select(
#     -contains("drop")
#   ) %>%
#   rename(
#     geographic_area_name = "Geographic Area Name"
#   )
df_q3
```

Use the following tests to check your work:

```{r q3-tests}
## NOTE: No need to edit
assertthat::assert_that(setequal(
  names(df_q3),
  c("id", "geographic_area_name", "category", "income_estimate", "income_moe")
))

assertthat::assert_that(
  df_q3 %>%
    filter(id == "0500000US01001", category == "2-person families") %>%
    pull(income_moe)
  == 6663
)

print("Nice!")
```

The data gives finite values for the Margin of Error, which is closely related to the Standard Error. The Census Bureau documentation gives the following relationship between Margin of Error and Standard Error:

$$\text{MOE} = 1.645 \times \text{SE}.$$

### **q4** Convert the margin of error to standard error. Additionally, compute a 99% confidence interval on income, and normalize the standard error to `income_CV = income_SE / income_estimate`. Provide these columns with the names `income_SE, income_lo, income_hi, income_CV`.

```{r q4-task}


df_q4 <- df_q3 %>%
  mutate(
    income_SE = income_moe/1.645,
    income_lo = income_estimate - income_SE*2.576,
    income_hi = income_estimate + income_SE*2.576,
    income_CV = income_SE/income_estimate
  )

df_q4

```

Use the following tests to check your work:

```{r q4-tests}
## NOTE: No need to edit
assertthat::assert_that(setequal(
  names(df_q4),
  c("id", "geographic_area_name", "category", "income_estimate", "income_moe",
    "income_SE", "income_lo", "income_hi", "income_CV")
))

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(id == "0500000US01001", category == "2-person families") %>%
    pull(income_SE) - 4050.456
  ) / 4050.456 < 1e-3
)

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(id == "0500000US01001", category == "2-person families") %>%
    pull(income_lo) - 54513.72
  ) / 54513.72 < 1e-3
)

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(id == "0500000US01001", category == "2-person families") %>%
    pull(income_hi) - 75380.28
  ) / 75380.28 < 1e-3
)

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(id == "0500000US01001", category == "2-person families") %>%
    pull(income_CV) - 0.06236556
  ) / 0.06236556 < 1e-3
)

print("Nice!")
```

One last wrangling step: We need to join the two datasets so we can compare population with income.

### **q5** Join `df_q4` and `df_pop`.

```{r q5-task}
## TODO: Join df_q4 and df_pop by the appropriate column

df_data <- right_join(df_pop, df_q4, by = c("id", "geographic_area_name"))


df_data 
```

# Analysis

<!-- ----------------------------------------------------------------------- -->

We now have both estimates and confidence intervals for `\\d-person families`. Now we can compare cases with quantified uncertainties: Let's practice!

### **q6** Study the following graph, making sure to note what you can *and can't* conclude based on the estimates and confidence intervals. Document your observations below and answer the questions.

```{r q6-task}
## NOTE: No need to edit; run and inspect
wid <- 0.5

df_data %>%
  filter(str_detect(geographic_area_name, "Massachusetts")) %>%
  mutate(
    county = str_remove(geographic_area_name, " County,.*$"),
    county = fct_reorder(county, income_estimate)
  ) %>%

  ggplot(aes(county, income_estimate, color = category)) +
  geom_errorbar(
    aes(ymin = income_lo, ymax = income_hi),
    position = position_dodge(width = wid)
  ) +
  geom_point(position = position_dodge(width = wid)) +

  coord_flip() +
  labs(
    x = "County",
    y = "Median Household Income"
  )
```

**Observations**:

-   Document your observations here.
    -   What's going on with 5 person families in Nantucket? Do I only have 1 observation or what? Why is the confidence interval so wide. It kind of checks out that we see wider confidence intervals as we reach 5/6 person families, because there would likely be fewer of those, yielding a less confident estimate of median household income.

    -   It appears like we can conclude a slight upward trend of median household income as families get bigger. Of course, this new income is split among more people, so we can't say anything about quality of life, merely that 6-person families appear to, as a whole, make more money than 2 person families.
-   Can you confidently distinguish between household incomes in Suffolk county? Why or why not?
    -   No. The way that the confidence intervals are arranged not only make it hard to distinguish between household incomes, but borderline impossible. For example, if I was looking at a household with a median income of 75000, it would be virtually impossible to categorize that because it falls inside the confidence interval of literally every kind of household in that county.
-   Which counties have the widest confidence intervals?
    -   This is a disingenuous question. On average? On average, I'd say Nantucket. It certainly seems like most of the confidence intervals for Nantucket are wide.

In the next task you'll investigate the relationship between population and uncertainty.

### **q7** Plot the standard error against population for all counties. Create a visual that effectively highlights the trends in the data. Answer the questions under *observations* below.

*Hint*: Remember that standard error is a function of *both* variability (e.g. variance) and sample size.

```{r q7-task}

df_data %>%
  ggplot(aes(x= population_estimate, y = income_SE)) + 
  geom_point()
```

**Observations**:

-   What *overall* trend do you see between `SE` and population? Why might this trend exist?
    -   It looks to generally be an equation with the form f(x) = 1/sqrt(x); this might make sense given the way that standard error is calculated: we see that sqrt(n) term appear in the denominator. This trend might exist because we might be selecting more people to sample from the total population in larger counties, because more people there are available and willing to be sampled.
-   What does this *overall* trend tell you about the relative ease of studying small vs large counties?
    -   It sure seems like it's easier to study large counties, which makes sense because you're more likely able to select a representative sample of the county without just selecting the entire population (or at least a huge part).

# Going Further

<!-- ----------------------------------------------------------------------- -->

Now it's your turn! You have income data for every county in the United States: Pose your own question and try to answer it with the data.

### **q8** Pose your own question about the data. Create a visualization (or table) here, and document your observations.

```{r q8-task}

df_ages<-
  df_income %>%
  select(
    id,
    contains("Geographic"),
    contains("median") & (matches("\\d\\d to \\d\\d years") | matches("\\d years and over"))
  ) %>%
  mutate(across(contains("median"), as.numeric)) %>%
  glimpse()

df_ages 

df_interim <- df_ages %>%
    pivot_longer(
    names_pattern = "(Estimate|Margin of Error).*(\\d\\d to \\d\\d years|\\d\\d years and over)",
    names_to = c(".value", "category"),
    cols = -c("id", "Geographic Area Name")
  ) %>%
  rename(
    "geographic_area_name" = "Geographic Area Name",
    "income_estimate" = "Estimate",
    "income_moe" =  "Margin of Error"
  ) %>%
  mutate(
    income_SE = income_moe/1.645,
    income_lo = income_estimate - income_SE*2.576,
    income_hi = income_estimate + income_SE*2.576,
    income_CV = income_SE/income_estimate
  ) 

df_q8 <- right_join(df_pop, df_interim, by = c("id", "geographic_area_name"))

df_q8 %>%
  ggplot(aes(x = category, y = income_estimate)) + 
  geom_boxplot()
## TODO: Pose and answer your own question about the data
```

QUESTION: How does average age of homeowner relate to average income in counties around the United States

**Observations**:

-   We see a peak in income_estimate when the homeowner is between 45-64 years old. Unsurprising, given that 45 years olds are, on average, more financially stable than when they were younger
-   What is shocking is that 15-24 year old homeowners have roughly the same income estimate as 65+ year old homeowners. Personally, I was shocked that the scale started at 15 years old --- presumably, many of these are inherited properties and so it's not shocking that they don't really have an income (indeed we see outliers that are pretty much zero).
-   I have a guess that people who are 65 years and older are likely mostly retired -- but I feel like their retirement fund still ought to be capable of giving them a comfortable living.
-   The data is also far tighter for 65+ year old homeowners... that makes sense, since it's a cultural thing to retire then.

Ideas:

-   Compare trends across counties that are relevant to you; e.g. places you've lived, places you've been, places in the US that are interesting to you.
-   In q3 we tidied the median `\\d-person families` columns only.
    -   Tidy the other median columns to learn about other people groups.
    -   Tidy the percentage columns to learn about how many households of each category are in each county.
-   Your own idea!

# References

<!-- ----------------------------------------------------------------------- -->

[1] Spielman SE, Folch DC, Nagle NN (2014) Patterns and causes of uncertainty in the American Community Survey. Applied Geography 46: 147--157. <pmid:25404783> [link](https://www.sciencedirect.com/science/article/pii/S0143622813002518?casa_token=VddzQ1-spHMAAAAA:FTq92LXgiPVloJUVjnHs8Ma1HwvPigisAYtzfqaGbbRRwoknNqZ6Y2IzszmGgIGH4JAPzQN0)
